{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4a3874a",
   "metadata": {},
   "source": [
    "# T5 - Book title generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076b3fb",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbf892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tommo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_metric,\n",
    "    Dataset\n",
    ")\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9785d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38501e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16559, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Animal Farm</td>\n",
       "      <td>Old Major, the old boar on the Manor Farm, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Clockwork Orange</td>\n",
       "      <td>Alex, a teenager living in near-future Englan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Plague</td>\n",
       "      <td>The text of The Plague is divided into five p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Title                                            Summary\n",
       "0         Animal Farm   Old Major, the old boar on the Manor Farm, ca...\n",
       "1  A Clockwork Orange   Alex, a teenager living in near-future Englan...\n",
       "2          The Plague   The text of The Plague is divided into five p..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./booksummaries.txt',\n",
    "                sep='\\t',\n",
    "                names=[\n",
    "                    'ID', 'Link', 'Title',\n",
    "                    'Author', 'PubDate', 'Genre', 'Summary'],\n",
    "                header=None,\n",
    "                usecols=['Title', 'Summary'])\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e7529f",
   "metadata": {},
   "source": [
    "### Data preprocessing, engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876a4f6",
   "metadata": {},
   "source": [
    "The data is already cleaned (downloaded from <a href=\"https://www.kaggle.com/datasets/athu1105/book-genre-prediction\">Kaggle</a>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1486cea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title      0\n",
       "Summary    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f1bc6",
   "metadata": {},
   "source": [
    "Create dataset and splitting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db936c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df[['Title', 'Summary']])\n",
    "ds = ds.shuffle(seed=42)\n",
    "ds = ds.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906da42a",
   "metadata": {},
   "source": [
    "Load tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab075d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,\n",
    "                                          model_max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65820871",
   "metadata": {},
   "source": [
    "Filter dataset based on summary length (<20 would be too low and >500 would be too much for the T5 model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c01e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3b60dc11874048afd1bb3343e39d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8e057897494b718fd8d6f90f651445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds['train'] = ds['train'].filter(\n",
    "    lambda example: (len(example['Summary']) >= 500) and\n",
    "    (len(example['Summary']) >= 20)\n",
    ")\n",
    "ds['test'] = ds['test'].filter(\n",
    "    lambda example: (len(example['Summary']) >= 500) and\n",
    "    (len(example['Summary']) >= 20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9990aa10",
   "metadata": {},
   "source": [
    "Encode inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18df42d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf0673414224a5ead6ffbcc308727d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c1318684e94f9a97f8417a71da31c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "def clean_text(text):\n",
    "    sentences = nltk.sent_tokenize(text.strip())\n",
    "    sentences_cleaned = [s for sent in sentences for s in sent.split(\"\\n\")]\n",
    "    sentences_cleaned_no_titles = [sent for sent in sentences_cleaned\n",
    "                                   if len(sent) > 0 and\n",
    "                                   sent[-1] in string.punctuation]\n",
    "    text_cleaned = \"\\n\".join(sentences_cleaned_no_titles)\n",
    "    return text_cleaned\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inputs = [f'generate title: {clean_text(text)}' for text in examples[\"Summary\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(examples[\"Title\"],\n",
    "                       max_length=max_target_length,\n",
    "                       truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ds = ds.map(preprocess_data, batched=True)\n",
    "tokenized_ds = tokenized_ds.remove_columns(['Summary', 'Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974300a",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba408327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\tommo/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\tommo/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # GPU limitation\n",
    "model_name = \"t5-base-book-title-generation-V1\"\n",
    "model_dir = f\"models/{model_name}\"\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                      for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "                      for label in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                      for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8fd5a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir '{model_dir}'/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9bf1b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\tommo/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\tommo/.cache\\huggingface\\hub\\models--t5-base\\snapshots\\23aa4f41cb7c08d4b05c8f327b22bfa0eb8c7ad9\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "c:\\users\\tommo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11177\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13975\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3712' max='13975' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3712/13975 3:45:56 < 10:25:00, 0.27 it/s, Epoch 1.33/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.391900</td>\n",
       "      <td>2.651303</td>\n",
       "      <td>31.698200</td>\n",
       "      <td>17.104500</td>\n",
       "      <td>31.507500</td>\n",
       "      <td>31.552600</td>\n",
       "      <td>5.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.888100</td>\n",
       "      <td>2.602286</td>\n",
       "      <td>32.498100</td>\n",
       "      <td>17.378800</td>\n",
       "      <td>32.215200</td>\n",
       "      <td>32.275000</td>\n",
       "      <td>5.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.735600</td>\n",
       "      <td>2.601072</td>\n",
       "      <td>32.298000</td>\n",
       "      <td>17.471600</td>\n",
       "      <td>32.065400</td>\n",
       "      <td>32.093100</td>\n",
       "      <td>5.243800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.837300</td>\n",
       "      <td>2.547202</td>\n",
       "      <td>32.798000</td>\n",
       "      <td>17.521600</td>\n",
       "      <td>32.576800</td>\n",
       "      <td>32.633100</td>\n",
       "      <td>5.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.721500</td>\n",
       "      <td>2.533113</td>\n",
       "      <td>32.750800</td>\n",
       "      <td>17.303800</td>\n",
       "      <td>32.483900</td>\n",
       "      <td>32.526600</td>\n",
       "      <td>5.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.572400</td>\n",
       "      <td>2.522356</td>\n",
       "      <td>32.924100</td>\n",
       "      <td>17.414400</td>\n",
       "      <td>32.564400</td>\n",
       "      <td>32.605700</td>\n",
       "      <td>5.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.767700</td>\n",
       "      <td>2.531556</td>\n",
       "      <td>33.622500</td>\n",
       "      <td>17.759000</td>\n",
       "      <td>33.362400</td>\n",
       "      <td>33.403200</td>\n",
       "      <td>5.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.634500</td>\n",
       "      <td>2.521943</td>\n",
       "      <td>32.988200</td>\n",
       "      <td>17.845400</td>\n",
       "      <td>32.752200</td>\n",
       "      <td>32.829500</td>\n",
       "      <td>5.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.638200</td>\n",
       "      <td>2.507135</td>\n",
       "      <td>33.624300</td>\n",
       "      <td>17.808900</td>\n",
       "      <td>33.295900</td>\n",
       "      <td>33.318000</td>\n",
       "      <td>5.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.682600</td>\n",
       "      <td>2.503758</td>\n",
       "      <td>33.179700</td>\n",
       "      <td>17.817200</td>\n",
       "      <td>32.908100</td>\n",
       "      <td>32.949300</td>\n",
       "      <td>5.418500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.451300</td>\n",
       "      <td>2.502378</td>\n",
       "      <td>33.194800</td>\n",
       "      <td>17.700200</td>\n",
       "      <td>32.872700</td>\n",
       "      <td>32.934700</td>\n",
       "      <td>5.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.767900</td>\n",
       "      <td>2.488652</td>\n",
       "      <td>33.571100</td>\n",
       "      <td>18.064700</td>\n",
       "      <td>33.320900</td>\n",
       "      <td>33.353600</td>\n",
       "      <td>5.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.635500</td>\n",
       "      <td>2.483117</td>\n",
       "      <td>33.709900</td>\n",
       "      <td>17.927100</td>\n",
       "      <td>33.454500</td>\n",
       "      <td>33.507500</td>\n",
       "      <td>5.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.729600</td>\n",
       "      <td>2.482761</td>\n",
       "      <td>33.784400</td>\n",
       "      <td>18.032500</td>\n",
       "      <td>33.501200</td>\n",
       "      <td>33.555000</td>\n",
       "      <td>5.449800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.750500</td>\n",
       "      <td>2.478719</td>\n",
       "      <td>33.678800</td>\n",
       "      <td>18.046900</td>\n",
       "      <td>33.392600</td>\n",
       "      <td>33.439300</td>\n",
       "      <td>5.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.632700</td>\n",
       "      <td>2.478928</td>\n",
       "      <td>33.722300</td>\n",
       "      <td>18.186200</td>\n",
       "      <td>33.429100</td>\n",
       "      <td>33.465700</td>\n",
       "      <td>5.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.725300</td>\n",
       "      <td>2.469491</td>\n",
       "      <td>33.580700</td>\n",
       "      <td>18.008700</td>\n",
       "      <td>33.303900</td>\n",
       "      <td>33.345300</td>\n",
       "      <td>5.577200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.481900</td>\n",
       "      <td>2.473724</td>\n",
       "      <td>33.658300</td>\n",
       "      <td>18.148300</td>\n",
       "      <td>33.420600</td>\n",
       "      <td>33.457800</td>\n",
       "      <td>5.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.545400</td>\n",
       "      <td>2.464003</td>\n",
       "      <td>33.934900</td>\n",
       "      <td>17.998900</td>\n",
       "      <td>33.674400</td>\n",
       "      <td>33.704300</td>\n",
       "      <td>5.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.531400</td>\n",
       "      <td>2.462920</td>\n",
       "      <td>34.120200</td>\n",
       "      <td>18.330800</td>\n",
       "      <td>33.814600</td>\n",
       "      <td>33.847600</td>\n",
       "      <td>5.579000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.759700</td>\n",
       "      <td>2.463804</td>\n",
       "      <td>33.415400</td>\n",
       "      <td>17.855200</td>\n",
       "      <td>33.165400</td>\n",
       "      <td>33.205300</td>\n",
       "      <td>5.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.592800</td>\n",
       "      <td>2.462931</td>\n",
       "      <td>34.088200</td>\n",
       "      <td>18.196600</td>\n",
       "      <td>33.866100</td>\n",
       "      <td>33.840200</td>\n",
       "      <td>5.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.611600</td>\n",
       "      <td>2.454818</td>\n",
       "      <td>33.683800</td>\n",
       "      <td>18.082200</td>\n",
       "      <td>33.488600</td>\n",
       "      <td>33.519600</td>\n",
       "      <td>5.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.601200</td>\n",
       "      <td>2.459272</td>\n",
       "      <td>33.856800</td>\n",
       "      <td>18.107600</td>\n",
       "      <td>33.604900</td>\n",
       "      <td>33.668100</td>\n",
       "      <td>5.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.628200</td>\n",
       "      <td>2.452412</td>\n",
       "      <td>33.889200</td>\n",
       "      <td>17.981400</td>\n",
       "      <td>33.635900</td>\n",
       "      <td>33.684500</td>\n",
       "      <td>5.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.678800</td>\n",
       "      <td>2.445739</td>\n",
       "      <td>33.694400</td>\n",
       "      <td>17.695000</td>\n",
       "      <td>33.421900</td>\n",
       "      <td>33.438000</td>\n",
       "      <td>5.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.583100</td>\n",
       "      <td>2.456925</td>\n",
       "      <td>34.104400</td>\n",
       "      <td>18.344300</td>\n",
       "      <td>33.849300</td>\n",
       "      <td>33.877000</td>\n",
       "      <td>5.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.610100</td>\n",
       "      <td>2.451566</td>\n",
       "      <td>33.992700</td>\n",
       "      <td>18.257200</td>\n",
       "      <td>33.761700</td>\n",
       "      <td>33.769700</td>\n",
       "      <td>5.536700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.450600</td>\n",
       "      <td>2.454363</td>\n",
       "      <td>34.369900</td>\n",
       "      <td>18.590000</td>\n",
       "      <td>34.098500</td>\n",
       "      <td>34.119000</td>\n",
       "      <td>5.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.386700</td>\n",
       "      <td>2.456939</td>\n",
       "      <td>34.209300</td>\n",
       "      <td>18.441000</td>\n",
       "      <td>33.953100</td>\n",
       "      <td>33.956300</td>\n",
       "      <td>5.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.413200</td>\n",
       "      <td>2.453159</td>\n",
       "      <td>34.230700</td>\n",
       "      <td>18.334000</td>\n",
       "      <td>33.953700</td>\n",
       "      <td>33.973900</td>\n",
       "      <td>5.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.369900</td>\n",
       "      <td>2.445437</td>\n",
       "      <td>34.528000</td>\n",
       "      <td>18.466200</td>\n",
       "      <td>34.296000</td>\n",
       "      <td>34.275600</td>\n",
       "      <td>5.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.382000</td>\n",
       "      <td>2.453477</td>\n",
       "      <td>34.152400</td>\n",
       "      <td>18.397500</td>\n",
       "      <td>33.928100</td>\n",
       "      <td>33.927500</td>\n",
       "      <td>5.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.346200</td>\n",
       "      <td>2.451050</td>\n",
       "      <td>34.261400</td>\n",
       "      <td>18.288000</td>\n",
       "      <td>34.040200</td>\n",
       "      <td>34.064300</td>\n",
       "      <td>5.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.434000</td>\n",
       "      <td>2.464467</td>\n",
       "      <td>33.367000</td>\n",
       "      <td>18.047100</td>\n",
       "      <td>33.166300</td>\n",
       "      <td>33.184800</td>\n",
       "      <td>5.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.435400</td>\n",
       "      <td>2.454541</td>\n",
       "      <td>33.774500</td>\n",
       "      <td>17.886200</td>\n",
       "      <td>33.502900</td>\n",
       "      <td>33.548200</td>\n",
       "      <td>5.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.426100</td>\n",
       "      <td>2.483777</td>\n",
       "      <td>32.973400</td>\n",
       "      <td>17.603400</td>\n",
       "      <td>32.777400</td>\n",
       "      <td>32.782700</td>\n",
       "      <td>5.517100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-100\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-100\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-100\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-100\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-100\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-200\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-200\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-200\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-200\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-300\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-300\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-300\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-300\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-300\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-300\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-400\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-400\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-400\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-400\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-400\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-500\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-500\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-500\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-500\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-600\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-600\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-600\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-600\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-600\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-600\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-700\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-700\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-700\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-700\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-700\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-700\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-800\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-800\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-800\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-800\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-800\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-800\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-900\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-900\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-900\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-900\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-900\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-900\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1000\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1000\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1000\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1000\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1100\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1100\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1100\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1100\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1100\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1100\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1200\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1200\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1200\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1200\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1200\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1200\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1300\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1300\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1300\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1300\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1300\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1300\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1400\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1400\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1400\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1400\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1400\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1400\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1500\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1500\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1500\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1500\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1600\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1600\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1600\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1600\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1600\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1600\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1700\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1700\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1700\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1700\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1700\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1700\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1800\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1800\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1800\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1800\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1800\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1800\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-1900\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-1900\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-1900\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-1900\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-1900\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-1900\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2000\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2000\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2000\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2000\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2100\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2100\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2100\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2100\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2100\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2100\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2200\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2200\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2200\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2200\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2200\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2200\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2300\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2300\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2300\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2300\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2300\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2400\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2400\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2400\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2400\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2400\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2400\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2500\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2500\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2500\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2500\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2600\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2600\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2600\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2600\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2600\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2600\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2700\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2700\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2700\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2700\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2700\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2700\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2800\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2800\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2800\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2800\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2800\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2800\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-2900\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-2900\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-2900\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-2900\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-2900\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-2900\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-3000\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-3000\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-3000\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-3000\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-3100\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-3100\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-3100\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-3100\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-3100\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-3100\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-3200\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-3200\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-3200\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-3200\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-3200\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-3200\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-2900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-3300\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-3300\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-3300\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-3300\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-3300\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-3300\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-3000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-3400\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-3400\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-3400\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-3400\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-3400\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-3400\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-3100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-3500\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-3500\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-3500\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-3500\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-3300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-3600\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-3600\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-3600\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-3600\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-3600\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-3600\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-3400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2810\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to models/t5-base-book-title-generation-V1\\checkpoint-3700\n",
      "Configuration saved in models/t5-base-book-title-generation-V1\\checkpoint-3700\\config.json\n",
      "Model weights saved in models/t5-base-book-title-generation-V1\\checkpoint-3700\\pytorch_model.bin\n",
      "tokenizer config file saved in models/t5-base-book-title-generation-V1\\checkpoint-3700\\tokenizer_config.json\n",
      "Special tokens file saved in models/t5-base-book-title-generation-V1\\checkpoint-3700\\special_tokens_map.json\n",
      "Copy vocab file to models/t5-base-book-title-generation-V1\\checkpoint-3700\\spiece.model\n",
      "Deleting older checkpoint [models\\t5-base-book-title-generation-V1\\checkpoint-3500] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2760/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\tommo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m         )\n\u001b[1;32m-> 1500\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1501\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tommo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1740\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1741\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1742\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                 if (\n",
      "\u001b[1;32mc:\\users\\tommo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2495\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_grad_scaling\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2496\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2497\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_apex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2498\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tommo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tommo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f99cb",
   "metadata": {},
   "source": [
    "### Test custom title generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./models/t5-base-book-title-generation-V1/checkpoint-3700/')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('./models/t5-base-book-title-generation-V1/checkpoint-3700/').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e4fd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x1bf800851c0>, 'http://127.0.0.1:7869/', None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_input(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "    return [f'generate title: {text}']\n",
    "\n",
    "def generate_title_from_summary(summary, top_p):\n",
    "    inputs = preprocess_input(summary)\n",
    "    inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\").to('cuda')\n",
    "    output = model.generate(**inputs, do_sample=True, max_length=50, top_p=top_p, top_k=0)\n",
    "    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "    return predicted_title\n",
    "\n",
    "summary = \"\"\"\n",
    "The story begins with Geralt of Rivia, Crown Princess Ciri of Cintra,\n",
    "and the sorceress Yennefer of Vengerberg at different points of time,\n",
    "exploring formative events that shape their characters throughout the first season,\n",
    "before eventually merging into a single timeline.\n",
    "Geralt and Ciri are linked by destiny since before she was born\n",
    "when he unknowingly demanded her as a reward for his services by invoking the Law of Surprise.\n",
    "After the two finally meet, Geralt becomes the princess's protector\n",
    "and must help her and fight against her various pursuers to prevent her Elder Blood\n",
    "and powerful magic from being used for malevolent purposes and keep Ciri and their world safe.\n",
    "\"\"\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_title_from_summary,\n",
    "    inputs=[gr.Textbox(value=summary.strip(), lines=3), gr.Slider(0, 1)],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
